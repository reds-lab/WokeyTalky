{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "import yaml\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def main(args):\n",
    "    # set seed\n",
    "    set_seed(0)\n",
    "\n",
    "    # Model, Tokenizer\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    access_token = \"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=access_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Data\n",
    "    json_file_path = os.path.join(args.dataset_file)\n",
    "\n",
    "    # Load the JSON file directly into the train dataset\n",
    "    train_dataset = Dataset.from_json(json_file_path)\n",
    "\n",
    "    # Create the DatasetDict with the loaded train dataset\n",
    "    ft_dict = DatasetDict({\"train\": train_dataset})\n",
    "\n",
    "    num_iters=3\n",
    "    # Training Arguments\n",
    "    output_dir = args.output_path\n",
    "    per_device_train_batch_size = 8\n",
    "    num_train_epochs = 10\n",
    "    gradient_accumulation_steps = 4\n",
    "    optim = \"paged_adamw_32bit\"\n",
    "    logging_strategy = \"epoch\"\n",
    "    learning_rate = 9.46183526073646e-07\n",
    "    max_grad_norm = 0.3\n",
    "    warmup_ratio = 0.03\n",
    "    lr_scheduler_type = \"constant\"\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        logging_strategy=logging_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        group_by_length=True,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "    )\n",
    "\n",
    "    # Initializing Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ft_dict['train'],\n",
    "        max_seq_length=1024,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments\n",
    "    )\n",
    "    for i in range(num_iters):\n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"{output_dir}/checkpoint-{(i+1)*num_train_epochs}\")\n",
    "    # Saving the model after 10 additional epochs\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Fine-tuning script')\n",
    "    parser.add_argument('--output_path', type=str, required=True, help='Path to save the fine-tuned model')\n",
    "    parser.add_argument('--project_name', type=str, required=True, help='Name of the project')\n",
    "    parser.add_argument('--dataset_file', type=str, required=True, help='Name of the dataset_path')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call the main function with the parsed arguments\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
